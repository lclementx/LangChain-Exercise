{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4Z2hqFxgirs",
        "outputId": "1307d0ad-5e02-4dcf-bf32-f4a88642cd28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.10/dist-packages (0.2.55)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.10.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2024.2.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.5.1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.11)\n",
            "Requirement already satisfied: langchain-experimental in /usr/local/lib/python3.10/dist-packages (0.0.53)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.38.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.28)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.25 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.27)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.29 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.30)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.23)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.29->langchain) (3.7.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.9.15)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.29->langchain) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.29->langchain) (1.2.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
            "downloading https://huggingface.co/TheBloke/Llama-2-13b-Chat-GGUF/resolve/main/llama-2-13b-chat.Q4_K_M.gguf to /root/.cache/huggingface/hub/tmp_ndqm8nm\n",
            "llama-2-13b-chat.Q4_K_M.gguf: 100% 7.87G/7.87G [01:12<00:00, 109MB/s]\n",
            "/content/llama-2-13b-chat.Q4_K_M.gguf\n"
          ]
        }
      ],
      "source": [
        "## INSTALLING DEPENDENCIES TO USE LLAMA-2-13b-CHAT for inference\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip3 install llama-cpp-python\n",
        "!pip3 install huggingface-hub\n",
        "!pip3 install sentence-transformers langchain langchain-experimental\n",
        "!huggingface-cli download TheBloke/Llama-2-13b-Chat-GGUF llama-2-13b-chat.Q4_K_M.gguf --local-dir /content --local-dir-use-symlinks False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
        "from langchain_community.document_loaders import AsyncHtmlLoader\n",
        "from langchain_community.document_loaders import BSHTMLLoader\n",
        "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from bs4 import BeautifulSoup\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
        "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
        "import pprint\n",
        "import re\n",
        "import json"
      ],
      "metadata": {
        "id": "3_T6Ol_rgm7J"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_gpu_layers = 40\n",
        "n_batch = 512\n",
        "\n",
        "##Initialize LLamma Model from GGUF file - use GPU for quicker inference\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"/content/llama-2-13b-chat.Q4_K_M.gguf\",\n",
        "    temperature=0,\n",
        "    n_gpu_layers=n_gpu_layers,\n",
        "    n_batch=n_batch,\n",
        "    n_ctx=2048,\n",
        "    verbose=True,\n",
        "    # grammar_path=\"/content/json.gbnf\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08nBjZLW-9xG",
        "outputId": "e053a62d-e194-42c2-d6f8-85c93632809f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from /content/llama-2-13b-chat.Q4_K_M.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   81 tensors\n",
            "llama_model_loader: - type q4_K:  241 tensors\n",
            "llama_model_loader: - type q6_K:   41 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 5120\n",
            "llm_load_print_meta: n_head           = 40\n",
            "llm_load_print_meta: n_head_kv        = 40\n",
            "llm_load_print_meta: n_layer          = 40\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
            "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 13824\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: model type       = 13B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 13.02 B\n",
            "llm_load_print_meta: model size       = 7.33 GiB (4.83 BPW) \n",
            "llm_load_print_meta: general.name     = LLaMA v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.28 MiB\n",
            "llm_load_tensors: offloading 40 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 40/41 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =  7500.85 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  7284.77 MiB\n",
            "....................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 2048\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  1600.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 1600.00 MiB, K (f16):  800.00 MiB, V (f16):  800.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host input buffer size   =    15.02 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   204.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    82.50 MiB\n",
            "llama_new_context_with_model: graph splits (measure): 3\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '40', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
            "Using fallback chat format: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###Create Pydantic BaseModel for better structured output\n",
        "class Company(BaseModel):\n",
        "  company_name: Optional[str] = Field(description=\"Company Name\")\n",
        "  company_domain: Optional[str] = Field(description=\"Company Domain\")\n",
        "\n",
        "class ArticleDetails(BaseModel):\n",
        "  related_companies: List[Company] = Field(description=\"Relevent Companies\")\n",
        "  topic: Optional[str] = Field(description=\"Topic\")\n",
        "\n",
        "##Get contents from HTML file\n",
        "file_path = \"/content/X is launching two new subscription tiers, including a ‘Premium+’ ad-free plan _ TechCrunch.html\"\n",
        "loader = BSHTMLLoader(file_path)\n",
        "data = loader.load()[0]\n",
        "data.page_content = re.sub(\"\\n\",\"\",data.page_content)\n",
        "\n",
        "###Get URLs from HTML file\n",
        "with open(\"/content/X is launching two new subscription tiers, including a ‘Premium+’ ad-free plan _ TechCrunch.html\", 'r') as file:\n",
        "    html_as_string = file.read()\n",
        "\n",
        "soup = BeautifulSoup(html_as_string, 'html.parser')\n",
        "a_tags = soup.find_all('a')\n",
        "urls = [element.get('href') for element in a_tags ]\n",
        "\n",
        "##Create Parser from JSON output\n",
        "output_parser = PydanticOutputParser(pydantic_object=ArticleDetails)\n",
        "format_instructions = output_parser.get_format_instructions()"
      ],
      "metadata": {
        "id": "AG4WnYJ7j_jA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_company_template = \"\"\"\n",
        "<s>[INST] <<SYS>>\n",
        "You are a helpful, respectful and honest business analyst.\n",
        "Always answer as helpfully as possible, while being safe.\n",
        "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
        "Please ensure that your responses are socially unbiased and positive in nature.\n",
        "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
        "If you don't know the answer to a question, please don't share false information.\n",
        "<</SYS>>\n",
        "Output a list of all the companies mentioned in the article.\n",
        "Article: {html}\n",
        "[/INST]\n",
        "\"\"\"\n",
        "\n",
        "prompt_url_template = \"\"\"<s>[INST] <<SYS>>\n",
        "You are a helpful, respectful and honest data analyst.\n",
        "Always answer as helpfully as possible, while being safe.\n",
        "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
        "Please ensure that your responses are socially unbiased and positive in nature.\n",
        "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
        "If you don't know the answer to a question, please don't share false information.\n",
        "\n",
        "Must only output JSON\n",
        "<</SYS>>\n",
        "\n",
        "Given the list of companies, find their websites from this list of URLs\n",
        "Companies: {companies}\n",
        "URL:{url_list}\n",
        "Output the answer in JSON in the following format {{\"related_companies\": [{{\"company_name\":company_name,\"company_domain\":domain}}]]}}.\n",
        "[/INST]\n",
        "\"\"\"\n",
        "\n",
        "prompt_topic_template = \"\"\"<s>[INST] <<SYS>>\n",
        "You are a helpful, respectful and honest data analyst.\n",
        "Always answer as helpfully as possible, while being safe.\n",
        "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
        "Please ensure that your responses are socially unbiased and positive in nature.\n",
        "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
        "If you don't know the answer to a question, please don't share false information.\n",
        "\n",
        "Must only output JSON\n",
        "<</SYS>>\n",
        "\n",
        "Given the article, give me the headline of the article.\n",
        "Output the answer in JSON in the following format {{\"topic\": \"topic of the article\"}}.\n",
        "Only must only output JSON\n",
        "Article: {html}\n",
        "[/INST]\n",
        "\"\"\"\n",
        "\n",
        "prompt_output_template = \"\"\"<s>[INST] <<SYS>>\n",
        "You are a helpful, respectful and honest data analyst.\n",
        "Always answer as helpfully as possible, while being safe.\n",
        "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
        "Please ensure that your responses are socially unbiased and positive in nature.\n",
        "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
        "If you don't know the answer to a question, please don't share false information.\n",
        "\n",
        "Must only output JSON\n",
        "<</SYS>>\n",
        "\n",
        "Join JSONs by key. Example: Json_1 = {{\"a\":[1,2,3]}} and Json_2 = {{\"b\":\"text\"}} output = {{\"a\":[1,2,3],\"b\":\"text\"}}\n",
        "\n",
        "Inputs\n",
        "JSON 1: {json_1}\n",
        "JSON 2: {json_2}\n",
        "\n",
        "Only output the result.\n",
        "[/INST]\n",
        "\"\"\"\n",
        "### Get the Topic\n",
        "prompt_topic = PromptTemplate(input_variables=['html'],template=prompt_topic_template)\n",
        "chain_topic = prompt_topic | llm | StrOutputParser()\n",
        "output_topic = chain_topic.invoke(input={'html':data.page_content}\n",
        "                                  # config={\"callbacks\": [ConsoleCallbackHandler()]}\n",
        "                                  )\n",
        "\n",
        "### Get Companies\n",
        "prompt_company = PromptTemplate(input_variables=['html','headline'], template=prompt_company_template)\n",
        "chain_company = prompt_company | llm | StrOutputParser()\n",
        "output_companies = chain_company.invoke(input={'html':data,'headline':output_topic})\n",
        "print(output_companies)\n",
        "\n",
        "### Get the URLs by passing in companies as llist and find the domains from hrefs extracted\n",
        "prompt_url = PromptTemplate(input_variables=['companies','url_list'],template=prompt_url_template)\n",
        "chain_url = {\"companies\": chain_company, \"url_list\": RunnableLambda(lambda x: x['url_list']) } \\\n",
        "            | prompt_url | llm | StrOutputParser()\n",
        "output_url = chain_url.invoke({'html':data,'url_list':urls}\n",
        "                              # config={\"callbacks\": [ConsoleCallbackHandler()]}\n",
        "                              )\n",
        "\n",
        "\n",
        "### Combine the JSONs\n",
        "prompt_output = PromptTemplate(input_variables=['json_1','json_2'],template=prompt_output_template)\n",
        "chain_output = prompt_output | llm | output_parser\n",
        "output = chain_output.invoke(input={'json_1':output_url,'json_2':output_topic})\n",
        "print(json.dumps(output.dict(),indent=2))"
      ],
      "metadata": {
        "id": "G7gY_LP9kgXw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6bf794e-e38d-4dff-f54f-447b7a31d7c7"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     452.18 ms\n",
            "llama_print_timings:      sample time =      18.55 ms /    31 runs   (    0.60 ms per token,  1671.07 tokens per second)\n",
            "llama_print_timings: prompt eval time =     659.57 ms /   695 tokens (    0.95 ms per token,  1053.72 tokens per second)\n",
            "llama_print_timings:        eval time =    1070.25 ms /    30 runs   (   35.68 ms per token,    28.03 tokens per second)\n",
            "llama_print_timings:       total time =    1857.05 ms /   725 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     452.18 ms\n",
            "llama_print_timings:      sample time =      21.93 ms /    36 runs   (    0.61 ms per token,  1641.51 tokens per second)\n",
            "llama_print_timings: prompt eval time =     681.55 ms /   896 tokens (    0.76 ms per token,  1314.66 tokens per second)\n",
            "llama_print_timings:        eval time =    1252.57 ms /    35 runs   (   35.79 ms per token,    27.94 tokens per second)\n",
            "llama_print_timings:       total time =    2080.33 ms /   931 tokens\n",
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is the list of companies mentioned in the article:\n",
            "\n",
            "1. X (formerly known as Twitter)\n",
            "2. Bloomberg\n",
            "3. Reuters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =     452.18 ms\n",
            "llama_print_timings:      sample time =      21.66 ms /    36 runs   (    0.60 ms per token,  1662.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_print_timings:        eval time =    1295.40 ms /    36 runs   (   35.98 ms per token,    27.79 tokens per second)\n",
            "llama_print_timings:       total time =    1448.34 ms /    37 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     452.18 ms\n",
            "llama_print_timings:      sample time =      76.51 ms /   115 runs   (    0.67 ms per token,  1503.11 tokens per second)\n",
            "llama_print_timings: prompt eval time =     628.54 ms /   716 tokens (    0.88 ms per token,  1139.14 tokens per second)\n",
            "llama_print_timings:        eval time =    4441.19 ms /   114 runs   (   38.96 ms per token,    25.67 tokens per second)\n",
            "llama_print_timings:       total time =    5642.70 ms /   830 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     452.18 ms\n",
            "llama_print_timings:      sample time =      86.08 ms /   141 runs   (    0.61 ms per token,  1638.11 tokens per second)\n",
            "llama_print_timings: prompt eval time =     267.60 ms /   221 tokens (    1.21 ms per token,   825.85 tokens per second)\n",
            "llama_print_timings:        eval time =    4861.21 ms /   140 runs   (   34.72 ms per token,    28.80 tokens per second)\n",
            "llama_print_timings:       total time =    5674.02 ms /   361 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"related_companies\": [\n",
            "    {\n",
            "      \"company_name\": \"X (formerly known as Twitter)\",\n",
            "      \"company_domain\": \"https://www.twitter.com\"\n",
            "    },\n",
            "    {\n",
            "      \"company_name\": \"Bloomberg\",\n",
            "      \"company_domain\": \"https://www.bloomberg.com\"\n",
            "    },\n",
            "    {\n",
            "      \"company_name\": \"Reuters\",\n",
            "      \"company_domain\": \"https://www.reuters.com\"\n",
            "    }\n",
            "  ],\n",
            "  \"topic\": \"X is launching two new subscription tiers, including a 'Premium+' ad-free plan\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l5jrLzdRQgxG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}